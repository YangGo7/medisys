import json
import sys
import os
import logging
from datetime import datetime
import importlib.util
import requests
from flask import Flask, request, jsonify
import pydicom
import io
import base64
import time
import traceback
from flask_cors import CORS


# ë¡œê¹… ì„¤ì •
# StreamHandlerì— ì¸ì½”ë”©ì„ ëª…ì‹œì ìœ¼ë¡œ 'utf-8'ë¡œ ì„¤ì •í•˜ì—¬ í•œê¸€ ê¹¨ì§ ë° UnicodeEncodeError ë°©ì§€
log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

file_handler = logging.FileHandler('/var/log/ai_service.log', encoding='utf-8')
file_handler.setFormatter(log_formatter)

stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setFormatter(log_formatter)
stream_handler.encoding = 'utf-8' # <-- ì´ ë¶€ë¶„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

logging.basicConfig(
    level=logging.INFO, # ìš´ì˜ í™˜ê²½ì—ì„œëŠ” INFO, ë””ë²„ê¹… ì‹œì—ëŠ” DEBUG
    handlers=[
        file_handler,
        stream_handler
    ]
)
logger = logging.getLogger('AIService')

# Flask ì•± ì´ˆê¸°í™”
app = Flask(__name__)
CORS(app, origins=["http://35.225.63.41:3000", "*"])  # ê°œë°œí™˜ê²½ìš©

# ê²½ë¡œ ë° ì„œë²„ ì„¤ì •
MODELS_PATH = '/models'
YOLO_MODEL_PATH = os.path.join(MODELS_PATH, 'yolov8', 'yolov8_inference.py')
SSD_MODEL_PATH = os.path.join(MODELS_PATH, 'ssd', 'ssd_inference.py')
ORTHANC_URL = os.getenv("ORTHANC_URL", "http://orthanc:8042")
DJANGO_URL = 'http://35.225.63.41:8000'

ORTHANC_USERNAME = 'orthanc'
ORTHANC_PASSWORD = 'orthanc'

auth_tuple = (ORTHANC_USERNAME, ORTHANC_PASSWORD)

class AIAnalyzer:
    MEDICAL_CLASSES = {
        0: 'Normal', 1: 'Aortic enlargement', 2: 'Atelectasis',
        3: 'Calcification', 4: 'Cardiomegaly', 5: 'Consolidation',
        6: 'ILD', 7: 'Infiltration', 8: 'Lung Opacity',
        9: 'Nodule/Mass', 10: 'Other lesion', 11: 'Pleural effusion',
        12: 'Pleural thickening', 13: 'Pulmonary fibrosis'
    }

    def __init__(self):
        self.yolo_module = None
        self.ssd_module = None
        self._load_models()

    def _load_models(self):
        try:
            if os.path.exists(YOLO_MODEL_PATH):
                spec = importlib.util.spec_from_file_location("yolo_inference", YOLO_MODEL_PATH)
                self.yolo_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(self.yolo_module)
                logger.info(f"YOLO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {YOLO_MODEL_PATH}")
            if os.path.exists(SSD_MODEL_PATH):
                spec = importlib.util.spec_from_file_location("ssd_inference", SSD_MODEL_PATH)
                self.ssd_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(self.ssd_module)
                logger.info(f"SSD ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {SSD_MODEL_PATH}")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(traceback.format_exc())
            
    def analyze_dicom_data(self, dicom_data, modality):
        start_time = time.time()
        try:
            if modality != 'CR':
                return self._create_mock_result(modality, skipped=True)

            # ğŸ”¥ ê°œë³„ ëª¨ë¸ ë¶„ì„ ë° ì €ì¥
            analysis_results = []

            # YOLO ëª¨ë¸ ë¶„ì„
            if self.yolo_module:
                try:
                    yolo_result = self.yolo_module.analyze(dicom_data)
                    if yolo_result.get('success'):
                        processing_time = time.time() - start_time
                        yolo_final = {
                            'success': True,
                            'detections': yolo_result.get('detections', []),
                            'model_type': 'yolo',
                            'metadata': {
                                'model_used': 'yolo',
                                'analysis_timestamp': datetime.now().isoformat(),
                                'modality': modality,
                                'processing_time': processing_time,
                                'model_version': 'yolov8_v1.0'
                            }
                        }
                        analysis_results.append(yolo_final)
                        logger.info(f"âœ… YOLO ë¶„ì„ ì„±ê³µ: {len(yolo_result.get('detections', []))}ê°œ íƒì§€")
                except Exception as e:
                    logger.error(f"YOLO ë¶„ì„ ì˜¤ë¥˜: {e}")
                    logger.error(traceback.format_exc())

            # SSD ëª¨ë¸ ë¶„ì„
            if self.ssd_module:
                try:
                    ssd_result = self.ssd_module.analyze(dicom_data)
                    if ssd_result.get('success'):
                        processing_time = time.time() - start_time
                        ssd_final = {
                            'success': True,
                            'detections': ssd_result.get('detections', []),
                            'model_type': 'ssd',
                            'metadata': {
                                'model_used': 'ssd',
                                'analysis_timestamp': datetime.now().isoformat(),
                                'modality': modality,
                                'processing_time': processing_time,
                                'model_version': 'ssd_v1.0'
                            }
                        }
                        analysis_results.append(ssd_final)
                        logger.info(f"âœ… SSD ë¶„ì„ ì„±ê³µ: {len(ssd_result.get('detections', []))}ê°œ íƒì§€")
                except Exception as e:
                    logger.error(f"SSD ë¶„ì„ ì˜¤ë¥˜: {e}")
                    logger.error(traceback.format_exc())

            return analysis_results

        except Exception as e:
            logger.error(f"DICOM ë¶„ì„ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return [self._create_error_result(str(e))]

    def _create_mock_result(self, modality, skipped=False):
        return [{
            'success': True,
            'detections': [],
            'message': f"{modality} ëª¨ë‹¬ë¦¬í‹° ë¶„ì„ ìŠ¤í‚µë¨.",
            'skipped': skipped,
            'metadata': {}
        }]

    def _create_error_result(self, error_msg):
        return [{
            'success': False,
            'error': error_msg,
            'detections': []
        }]

ai_analyzer = AIAnalyzer()

def resolve_instance_id(uid_or_id):
    """
    ë‚´ë¶€ UUID ë˜ëŠ” SOPInstanceUID ë‘˜ ë‹¤ ì²˜ë¦¬
    """
    if '-' in uid_or_id:
        return uid_or_id  # UUID í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ
    else:
        try:
            find_url = f"{ORTHANC_URL}/tools/find"
            query = {
                "Level": "Instance",
                "Query": {
                    "SOPInstanceUID": uid_or_id
                }
            }
            response = requests.post(find_url, auth=auth_tuple, json=query, timeout=10)
            response.raise_for_status()
            matches = response.json()
            if matches:
                return matches[0]  # ì²« ë²ˆì§¸ ë§¤ì¹­ ID
        except Exception as e:
            logger.warning(f"UID ë§¤í•‘ ì‹¤íŒ¨: {uid_or_id}, ì´ìœ : {e}")
    return None


@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'healthy', 'timestamp': datetime.now().isoformat()})

@app.route('/analyze/<instance_id>', methods=['POST'])
def analyze_instance(instance_id):
    try:
        # SOPInstanceUID â†’ Orthanc ë‚´ë¶€ ID ë§¤í•‘
        resolved_id = resolve_instance_id(instance_id)
        if not resolved_id:
            return jsonify({'error': 'Instance not found'}), 404

        logger.info(f"ì¸ìŠ¤í„´ìŠ¤ ë¶„ì„ ìš”ì²­ (Resolved ID: {resolved_id})")

        # â›” ê¸°ì¡´ instance_id â†’ âœ… resolved_id ì‚¬ìš©
        instance_info = get_instance_info(resolved_id)
        if not instance_info:
            return jsonify({'error': 'Instance info not found'}), 404

        dicom_data = get_dicom_file(resolved_id)
        if not dicom_data:
            return jsonify({'error': 'DICOM download failed'}), 500

        modality = instance_info['MainDicomTags'].get('Modality', 'UNKNOWN')
        image_width, image_height = get_image_dimensions_from_data(dicom_data)

        analysis_results = ai_analyzer.analyze_dicom_data(dicom_data, modality)

        saved_results = []
        for result in analysis_results:
            if result.get('success') and not result.get('skipped'):
                result['metadata'] = result.get('metadata', {})
                result['metadata']['image_width'] = image_width
                result['metadata']['image_height'] = image_height

                # ì—¬ì „íˆ ì €ì¥ì€ original instance_id ì‚¬ìš© (UID ì €ì¥ ëª©ì )
                save_result = save_analysis_result(instance_id, result, dicom_data)
                saved_results.append(save_result)

        return jsonify({
            'success': True,
            'total_models': len(analysis_results),
            'saved_count': len(saved_results),
            'results': analysis_results,
            'image_dimensions': f"{image_width}x{image_height}"
        })

    except Exception as e:
        logger.error(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

# ğŸ”¥ ìƒˆë¡œìš´ í•¨ìˆ˜: DICOM ë°ì´í„°ì—ì„œ ì§ì ‘ í•´ìƒë„ ì¶”ì¶œ
def get_image_dimensions_from_data(dicom_data):
    """DICOM ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì—ì„œ í•´ìƒë„ ì •ë³´ ì¶”ì¶œ"""
    try:
        import pydicom
        import io
        
        dicom_file = pydicom.dcmread(io.BytesIO(dicom_data))
        
        # Rows, Columns íƒœê·¸ì—ì„œ í•´ìƒë„ ì¶”ì¶œ
        height = getattr(dicom_file, 'Rows', None)
        width = getattr(dicom_file, 'Columns', None)
        
        if height and width:
            logger.info(f"ğŸ“ DICOMì—ì„œ í•´ìƒë„ ì¶”ì¶œ: {width}x{height}")
            return int(width), int(height)
        else:
            logger.warning(f"âš ï¸ DICOMì—ì„œ í•´ìƒë„ ì •ë³´ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return 1024, 1024
            
    except Exception as e:
        logger.error(f"âŒ DICOM í•´ìƒë„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return 1024, 1024
    

@app.route('/results/<instance_id>', methods=['GET'])
def get_analysis_result(instance_id):
    try:
        # âœ… ë¨¼ì € ë‚´ë¶€ Orthanc IDë¡œ ë³€í™˜ (SOPInstanceUID ì§€ì›)
        resolved_id = resolve_instance_id(instance_id)
        if not resolved_id:
            return jsonify({'error': 'Instance not found'}), 404

        instance_info = get_instance_info(resolved_id)
        if not instance_info:
            return jsonify({'error': 'Instance info not found'}), 404

        study_uid = instance_info.get('MainDicomTags', {}).get('StudyInstanceUID')
        if not study_uid:
            return jsonify({'error': 'Study UID not found'}), 404

        url = f"{DJANGO_URL}/api/ai/results/save/{study_uid}/"
        logger.info(f"ğŸ“¥ ê²°ê³¼ ì¡°íšŒ URL: {url}")
        resp = requests.get(url, timeout=80)

        if resp.status_code == 200:
            return jsonify(resp.json())

        return jsonify({'error': 'Failed to fetch results', 'details': resp.text}), resp.status_code

    except Exception as e:
        logger.error(f"âŒ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500


def get_instance_info(instance_id):
    try:
        # âœ… ë¨¼ì € instance_idê°€ SOPInstanceUIDì¸ì§€ í™•ì¸í•˜ê³  Orthanc ë‚´ë¶€ IDë¡œ ë³€í™˜
        if '-' not in instance_id:  # SOPInstanceUIDëŠ” ë³´í†µ '-' ì—†ìŒ
            logger.info(f"ğŸ” SOPInstanceUID ê°ì§€ë¨, ë‚´ë¶€ IDë¡œ ë§¤í•‘ ì‹œë„: {instance_id}")
            resolved = resolve_instance_id(instance_id)
            if not resolved:
                logger.warning(f"âŒ SOPInstanceUIDë¥¼ ë‚´ë¶€ Orthanc IDë¡œ ë§¤í•‘ ì‹¤íŒ¨: {instance_id}")
                return None
            instance_id = resolved  # ë§¤í•‘ ì„±ê³µ ì‹œ ë‚´ë¶€ IDë¡œ êµì²´

        # ğŸ” ì‹¤ì œ ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ìš”ì²­
        r1 = requests.get(f"{ORTHANC_URL}/instances/{instance_id}", auth=auth_tuple, timeout=80)
        logger.info(f"â¡ï¸ GET {ORTHANC_URL}/instances/{instance_id}")
        logger.info(f"â¡ï¸ ì‘ë‹µ ì½”ë“œ: {r1.status_code}")
        if r1.status_code != 200:
            logger.warning(f"Orthanc ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {instance_id}, Status: {r1.status_code}, Response: {r1.text}")
            return None
        instance_info = r1.json()

        # ğŸ” Simplified íƒœê·¸ë„ ë³‘í•©
        r2 = requests.get(f"{ORTHANC_URL}/instances/{instance_id}/simplified-tags", auth=auth_tuple, timeout=80)
        if r2.status_code == 200:
            tags = r2.json()
            instance_info.setdefault('MainDicomTags', {})
            for key in ['PatientID', 'StudyInstanceUID', 'SeriesInstanceUID', 'instance_id', 'InstanceNumber', 'Modality']:
                if key in tags:
                    instance_info['MainDicomTags'][key] = tags[key]

        # âœ… í•„ìˆ˜ íƒœê·¸ ê²€ì¦
        required_keys = ['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID']
        for key in required_keys:
            if not instance_info['MainDicomTags'].get(key):
                logger.warning(f"{key}ê°€ ëˆ„ë½ë¨. Instance ID: {instance_id}")
                return None

        return instance_info

    except Exception as e:
        logger.error(f"get_instance_info ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return None




def get_dicom_file(instance_id):
    try:
        r = requests.get(f"{ORTHANC_URL}/instances/{instance_id}/file", auth=auth_tuple, timeout=80)
        if r.status_code == 200:
            return r.content
        logger.warning(f"DICOM íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {instance_id}, Status: {r.status_code}, Response: {r.text}")
        return None
    except Exception as e:
        logger.error(f"get_dicom_file ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return None

# ğŸ”¥ ìˆ˜ì •ëœ save_analysis_result í•¨ìˆ˜ (dicom_data ë§¤ê°œë³€ìˆ˜ ì¶”ê°€)
def save_analysis_result(instance_id, result_dict, dicom_data=None):
    try:
        # bboxë¥¼ x, y, width, height í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
        def normalize_bbox(bbox_data):
            logger.debug(f"ğŸ” normalize_bbox í˜¸ì¶œ: {bbox_data} (type: {type(bbox_data)})")
            if isinstance(bbox_data, dict):
                # case 1: {'x':..., 'y':..., 'width':..., 'height':...}
                if all(k in bbox_data for k in ['x', 'y', 'width', 'height']):
                    return {
                        "x": int(bbox_data.get("x", 0)),
                        "y": int(bbox_data.get("y", 0)),
                        "width": int(bbox_data.get("width", 0)),
                        "height": int(bbox_data.get("height", 0)),
                    }
                # case 2: {'x1':..., 'y1':..., 'x2':..., 'y2':...}
                elif all(k in bbox_data for k in ['x1', 'y1', 'x2', 'y2']):
                    x1 = bbox_data.get("x1", 0)
                    y1 = bbox_data.get("y1", 0)
                    x2 = bbox_data.get("x2", 0)
                    y2 = bbox_data.get("y2", 0)
                    return {
                        "x": int(x1),
                        "y": int(y1),
                        "width": int(x2 - x1),
                        "height": int(y2 - y1),
                    }
            # case 3: [x1, y1, x2, y2] (list/tuple)
            elif isinstance(bbox_data, (list, tuple)) and len(bbox_data) == 4:
                x1, y1, x2, y2 = bbox_data
                return {
                    "x": int(x1),
                    "y": int(y1),
                    "width": int(x2 - x1),
                    "height": int(y2 - y1),
                }
            # Torchvision Tensor (ì˜ˆ: torch.Tensor([[x1,y1,x2,y2]]))
            elif hasattr(bbox_data, 'tolist') and isinstance(bbox_data.tolist(), list) and len(bbox_data.tolist()) == 4:
                x1, y1, x2, y2 = bbox_data.tolist()
                logger.debug(f"ğŸ” bbox_dataê°€ torch.Tensor ë¼ì´í¬ ê°ì²´ì„: {x1, y1, x2, y2}")
                return {
                    "x": int(x1),
                    "y": int(y1),
                    "width": int(x2 - x1),
                    "height": int(y2 - y1),
                }
            
            # ëª¨ë“  ê²½ìš°ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
            logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” bbox í˜•ì‹: {bbox_data} (type: {type(bbox_data)}). ê¸°ë³¸ê°’ ì‚¬ìš©.")
            return {"x": 0, "y": 0, "width": 0, "height": 0}
        
        # ğŸ”¥ í•´ìƒë„ ì •ë³´ íšë“ (ì´ë¯¸ ì¶”ì¶œëœ ê²ƒ ìš°ì„  ì‚¬ìš©)
        model_width = result_dict.get("metadata", {}).get("image_width")
        model_height = result_dict.get("metadata", {}).get("image_height")
        
        # ğŸ”¥ DICOM ë°ì´í„°ê°€ ì „ë‹¬ëœ ê²½ìš°ì—ë§Œ ì¶”ì¶œ (fallback)
        dicom_width, dicom_height = None, None
        if not model_width or not model_height:
            if dicom_data:
                dicom_width, dicom_height = get_image_dimensions_from_data(dicom_data)
            
        # ìš°ì„ ìˆœìœ„: ëª¨ë¸ ì œê³µ > DICOM ì¶”ì¶œ > ê¸°ë³¸ê°’
        final_width = model_width or dicom_width or 1024
        final_height = model_height or dicom_height or 1024
        
        logger.info(f"ğŸ“ ìµœì¢… í•´ìƒë„: {final_width}x{final_height} (ëª¨ë¸:{model_width}x{model_height}, DICOM:{dicom_width}x{dicom_height})")

        detections = []
        raw_detections = result_dict.get("detections", [])

        logger.debug(f"ğŸ” save_analysis_result ì‹œì‘. raw_detections íƒ€ì…: {type(raw_detections)}, ë‚´ìš©: {raw_detections}")

        # raw_detectionsê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ê³ , ì•„ë‹ˆë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        if not isinstance(raw_detections, list):
            if isinstance(raw_detections, str):
                try:
                    raw_detections = json.loads(raw_detections)
                    logger.debug(f"âœ… raw_detections ë¬¸ìì—´ â†’ dict íŒŒì‹±ë¨: {raw_detections}")
                    if not isinstance(raw_detections, list):
                         logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ JSON íŒŒì‹± í›„ì—ë„ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ì‹¤ì œ íƒ€ì…: {type(raw_detections)}")
                         raw_detections = []
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©°, JSON íŒŒì‹± ì‹¤íŒ¨: {e} - ì›ë³¸: '{raw_detections}'")
                    raw_detections = []
                except Exception as e:
                    logger.error(f"âŒ 'detections' í‚¤ ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì‹± ì˜¤ë¥˜: {e} - ì›ë³¸: '{raw_detections}'")
                    raw_detections = []
            else:
                logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ì‹¤ì œ íƒ€ì…: {type(raw_detections)}. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”.")
                raw_detections = []

        for i, item in enumerate(raw_detections):
            logger.debug(f"ğŸ“¦ í˜„ì¬ {i}ë²ˆì§¸ item: {item} (type: {type(item)})")

            # ë¬¸ìì—´ì¼ ê²½ìš° JSON íŒŒì‹± ì‹œë„ (ê° itemë³„ë¡œ)
            if isinstance(item, str):
                try:
                    item = json.loads(item)
                    logger.debug(f"âœ… {i}ë²ˆì§¸ item ë¬¸ìì—´ â†’ dict íŒŒì‹±ë¨: {item}")
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ {i}ë²ˆì§¸ item JSON íŒŒì‹± ì‹¤íŒ¨: {e} - ì›ë³¸ ë¬¸ìì—´: '{item}'")
                    continue
                except Exception as e:
                    logger.error(f"âŒ {i}ë²ˆì§¸ item ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì‹± ì˜¤ë¥˜: {e} - ì›ë³¸ ë¬¸ìì—´: '{item}'")
                    continue

            elif hasattr(item, 'keys') and callable(getattr(item, 'keys')):
                logger.debug(f"âœ… {i}ë²ˆì§¸ itemì€ dictì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤. í‚¤ í™•ì¸: {list(item.keys())}")
            elif hasattr(item, 'boxes') and hasattr(item, 'labels') and hasattr(item, 'scores'):
                logger.debug(f"âœ… {i}ë²ˆì§¸ itemì€ ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ ê°ì²´ (boxes, labels, scores í¬í•¨).")
                continue

            # ì—¬ì „íˆ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë©´ ë¬´ì‹œ
            if not isinstance(item, dict):
                logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì´ ìµœì¢…ì ìœ¼ë¡œ dictê°€ ì•„ë‹˜, ë¬´ì‹œ: {item} (type: {type(item)})")
                continue

            # bbox ì²˜ë¦¬ (normalize_bbox í•¨ìˆ˜ ì‚¬ìš©)
            normalized_bbox = normalize_bbox(item.get("bbox"))
            if normalized_bbox is None: 
                 logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì˜ bbox ì •ê·œí™” ì‹¤íŒ¨. ê¸°ë³¸ê°’ ì‚¬ìš©.")
                 normalized_bbox = {"x": 0, "y": 0, "width": 0, "height": 0}

            # í´ë˜ìŠ¤ ì´ë¦„, í™•ì‹ ë„, ì„¤ëª… í•„ë“œ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
            class_name = item.get("label") or item.get("class_name", "")
            confidence = item.get("confidence_score") or item.get("confidence", 0.0)
            description = item.get("ai_text") or item.get("description", "")
            
            # Ensure confidence is float
            try:
                confidence = float(confidence)
            except (ValueError, TypeError):
                confidence = 0.0
                logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì˜ confidence ê°’ì´ ìˆ«ìê°€ ì•„ë‹˜: {item.get('confidence_score') or item.get('confidence')}. 0.0ìœ¼ë¡œ ì„¤ì •.")

            detections.append({
                "class_name": class_name, 
                "confidence": confidence,
                "bbox": normalized_bbox,
                "description": description
            })

        # ğŸ”¥ í•´ìƒë„ ì •ë³´ë¥¼ í¬í•¨í•œ payload ìƒì„±
        payload = {
            "instance_id": instance_id,
            "model_type": result_dict.get("model_type", "unknown"),
            "result": {
                "detections": detections,
                "metadata": {
                    **result_dict.get("metadata", {}),
                    # ğŸ”¥ í•´ìƒë„ ì •ë³´ ì¶”ê°€
                    "image_width": final_width,
                    "image_height": final_height
                },
                "processing_time": float(result_dict.get("processing_time", 0.0)),
                # ğŸ”¥ ìµœìƒìœ„ ë ˆë²¨ì—ë„ í•´ìƒë„ ì •ë³´ ì¶”ê°€ (Django API í˜¸í™˜ì„±)
                "image_width": final_width,
                "image_height": final_height
            }
        }

        url = f"{DJANGO_URL}/api/ai/results/save/"
        headers = {'Content-Type': 'application/json'}

        logger.info(f"ğŸ“¡ Djangoì— {result_dict.get('model_type', 'unknown')} ë¶„ì„ ê²°ê³¼ ì „ì†¡: {instance_id} (í•´ìƒë„: {final_width}x{final_height})")
        resp = requests.post(url, json=payload, headers=headers, timeout=15)

        if resp.status_code in [200, 201]:
            try:
                response_json = resp.json()
                count = response_json.get('count', 0)
                logger.info(f"âœ… {result_dict.get('model_type', 'unknown')} ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {count}ê±´, ì‘ë‹µ: {response_json}")
                return response_json
            except json.JSONDecodeError:
                logger.info(f"âœ… {result_dict.get('model_type', 'unknown')} ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ, Django ì‘ë‹µì€ JSON í˜•ì‹ì´ ì•„ë‹˜: {resp.text}")
                return {"status": "success", "message": resp.text}
        else:
            logger.warning(f"âš ï¸ Django ì „ì†¡ ì‹¤íŒ¨: {resp.status_code} - {resp.text}")
            return {"status": "error", "message": resp.text}

    except Exception as e:
        logger.error(f"ğŸ”¥ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        logger.error(traceback.format_exc())
        return {"status": "error", "message": str(e)}


def analyze_single_instance(instance_id):
    instance_info = get_instance_info(instance_id)
    if not instance_info:
        raise Exception("Instance not found")

    dicom_data = get_dicom_file(instance_id)
    if not dicom_data:
        raise Exception("DICOM file download failed")

    # í•´ìƒë„ ì •ë³´ ë¯¸ë¦¬ ì¶”ì¶œ
    image_width, image_height = get_image_dimensions_from_data(dicom_data)

    modality = instance_info['MainDicomTags'].get('Modality', 'UNKNOWN')
    results = ai_analyzer.analyze_dicom_data(dicom_data, modality)
    
    # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸ (analyze_dicom_dataëŠ” ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
    if not isinstance(results, list):
        results = [results]
    
    # ê° ê²°ê³¼ì— í•´ìƒë„ ì •ë³´ ì¶”ê°€í•˜ê³  ì €ì¥
    for result in results:
        if result and result.get('success'):
            # í•´ìƒë„ ì •ë³´ ì¶”ê°€
            result['metadata'] = result.get('metadata', {})
            result['metadata']['image_width'] = image_width
            result['metadata']['image_height'] = image_height
            
            save_analysis_result(instance_id, result, dicom_data)
        else:
            raise Exception(f"AI analysis failed: {result.get('error', 'unknown') if result else 'No result'}")

    return results[0] if len(results) == 1 else results


@app.route('/analyze-study/<study_uid>', methods=['POST'])
def analyze_study_by_uid(study_uid):
    """
    StudyInstanceUID (DICOM UID)ë¡œë¶€í„° ë¶„ì„ ì‹¤í–‰ (í”„ë¡ íŠ¸ì—”ë“œ ëŒ€ì‘)
    """
    try:
        logger.info(f"ğŸ“© studyUID ë¶„ì„ ìš”ì²­ ë„ì°©: {study_uid}")
        
        # 1. study_uid â†’ ë‚´ë¶€ Orthanc Study ID ì¡°íšŒ
        find_url = f"{ORTHANC_URL}/tools/find"
        query = {
            "Level": "Study",
            "Query": {
                "StudyInstanceUID": study_uid
            }
        }
        find_response = requests.post(find_url, auth=auth_tuple, json=query, timeout=10)
        if find_response.status_code != 200 or not find_response.json():
            logger.error(f"âŒ studyUID {study_uid} ì— í•´ë‹¹í•˜ëŠ” ìŠ¤í„°ë””ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return jsonify({"error": "Study UID not found in Orthanc"}), 404

        orthanc_study_id = find_response.json()[0]
        logger.info(f"âœ… studyUID â†’ ë‚´ë¶€ Orthanc ID ë§¤í•‘ ì„±ê³µ: {orthanc_study_id}")

        # 2. í•´ë‹¹ ìŠ¤í„°ë””ì˜ ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ
        series_list = requests.get(f"{ORTHANC_URL}/studies/{orthanc_study_id}/series", auth=auth_tuple).json()
        if not series_list:
            return jsonify({'error': 'No series found in study'}), 404

        instance_ids = []
        for series in series_list:
            series_id = series.get("ID")
            instances = requests.get(f"{ORTHANC_URL}/series/{series_id}/instances", auth=auth_tuple).json()
            instance_ids.extend([inst["ID"] for inst in instances])

        if not instance_ids:
            return jsonify({'error': 'No instances found in study'}), 404

        logger.info(f"ğŸ” ì´ {len(instance_ids)}ê°œì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤")

        # 3. ê° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë¶„ì„
        all_results = []
        for instance_id in instance_ids:
            try:
                result = analyze_single_instance(instance_id)
                all_results.append({
                    "instance_id": instance_id,
                    "result": result
                })
            except Exception as e:
                logger.warning(f"âš ï¸ ì¸ìŠ¤í„´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {instance_id}, ì›ì¸: {e}")

        return jsonify({
            "study_uid": study_uid,
            "orthanc_study_id": orthanc_study_id,
            "analyzed_instances": len(all_results),
            "results": all_results
        })

    except Exception as e:
        logger.error(f"ğŸ”¥ Study ë¶„ì„ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    logger.info("ğŸš€ Flask ì„œë²„ ì‹œì‘ë¨ - í¬íŠ¸ 5000, ë””ë²„ê¹… ëª¨ë“œ ë¹„í™œì„±í™”")
    app.run(host="0.0.0.0", port=5000, debug=False)
