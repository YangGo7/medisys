import json
import sys
import os
import logging
from datetime import datetime
import importlib.util
import requests
from flask import Flask, request, jsonify
import pydicom
import io
import base64
import time
import traceback
from flask_cors import CORS


# ë¡œê¹… ì„¤ì •
# StreamHandlerì— ì¸ì½”ë”©ì„ ëª…ì‹œì ìœ¼ë¡œ 'utf-8'ë¡œ ì„¤ì •í•˜ì—¬ í•œê¸€ ê¹¨ì§ ë° UnicodeEncodeError ë°©ì§€
log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

file_handler = logging.FileHandler('/var/log/ai_service.log', encoding='utf-8')
file_handler.setFormatter(log_formatter)

stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setFormatter(log_formatter)
stream_handler.encoding = 'utf-8' # <-- ì´ ë¶€ë¶„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

logging.basicConfig(
    level=logging.INFO, # ìš´ì˜ í™˜ê²½ì—ì„œëŠ” INFO, ë””ë²„ê¹… ì‹œì—ëŠ” DEBUG
    handlers=[
        file_handler,
        stream_handler
    ]
)
logger = logging.getLogger('AIService')

# Flask ì•± ì´ˆê¸°í™”
app = Flask(__name__)
CORS(app, origins=["http://35.225.63.41:3000", "*"])  # ê°œë°œí™˜ê²½ìš©

# ê²½ë¡œ ë° ì„œë²„ ì„¤ì •
MODELS_PATH = '/models'
YOLO_MODEL_PATH = os.path.join(MODELS_PATH, 'yolov8', 'yolov8_inference.py')
SSD_MODEL_PATH = os.path.join(MODELS_PATH, 'ssd', 'ssd_inference.py')
ORTHANC_URL = os.getenv("ORTHANC_URL", "http://35.225.63.41:8042")
DJANGO_URL = 'http://35.225.63.41:8000'

ORTHANC_USERNAME = 'orthanc'
ORTHANC_PASSWORD = 'orthanc'

auth_tuple = (ORTHANC_USERNAME, ORTHANC_PASSWORD)

class AIAnalyzer:
    MEDICAL_CLASSES = {
        0: 'Normal', 1: 'Aortic enlargement', 2: 'Atelectasis',
        3: 'Calcification', 4: 'Cardiomegaly', 5: 'Consolidation',
        6: 'ILD', 7: 'Infiltration', 8: 'Lung Opacity',
        9: 'Nodule/Mass', 10: 'Other lesion', 11: 'Pleural effusion',
        12: 'Pleural thickening', 13: 'Pulmonary fibrosis'
    }

    def __init__(self):
        self.yolo_module = None
        self.ssd_module = None
        self._load_models()

    def _load_models(self):
        try:
            if os.path.exists(YOLO_MODEL_PATH):
                spec = importlib.util.spec_from_file_location("yolo_inference", YOLO_MODEL_PATH)
                self.yolo_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(self.yolo_module)
                logger.info(f"YOLO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {YOLO_MODEL_PATH}")
            if os.path.exists(SSD_MODEL_PATH):
                spec = importlib.util.spec_from_file_location("ssd_inference", SSD_MODEL_PATH)
                self.ssd_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(self.ssd_module)
                logger.info(f"SSD ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {SSD_MODEL_PATH}")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(traceback.format_exc())
            
    def analyze_dicom_data(self, dicom_data, modality):
        start_time = time.time()
        try:
            if modality != 'CR':
                return self._create_mock_result(modality, skipped=True)

            # ğŸ”¥ ê°œë³„ ëª¨ë¸ ë¶„ì„ ë° ì €ì¥
            analysis_results = []

            # YOLO ëª¨ë¸ ë¶„ì„
            if self.yolo_module:
                try:
                    yolo_result = self.yolo_module.analyze(dicom_data)
                    if yolo_result.get('success'):
                        processing_time = time.time() - start_time
                        yolo_final = {
                            'success': True,
                            'detections': yolo_result.get('detections', []),
                            'model_type': 'yolo',
                            'metadata': {
                                'model_used': 'yolo',
                                'analysis_timestamp': datetime.now().isoformat(),
                                'modality': modality,
                                'processing_time': processing_time,
                                'model_version': 'yolov8_v1.0'
                            }
                        }
                        analysis_results.append(yolo_final)
                        logger.info(f"âœ… YOLO ë¶„ì„ ì„±ê³µ: {len(yolo_result.get('detections', []))}ê°œ íƒì§€")
                except Exception as e:
                    logger.error(f"YOLO ë¶„ì„ ì˜¤ë¥˜: {e}")
                    logger.error(traceback.format_exc())

            # SSD ëª¨ë¸ ë¶„ì„
            if self.ssd_module:
                try:
                    ssd_result = self.ssd_module.analyze(dicom_data)
                    if ssd_result.get('success'):
                        processing_time = time.time() - start_time
                        ssd_final = {
                            'success': True,
                            'detections': ssd_result.get('detections', []),
                            'model_type': 'ssd',
                            'metadata': {
                                'model_used': 'ssd',
                                'analysis_timestamp': datetime.now().isoformat(),
                                'modality': modality,
                                'processing_time': processing_time,
                                'model_version': 'ssd_v1.0'
                            }
                        }
                        analysis_results.append(ssd_final)
                        logger.info(f"âœ… SSD ë¶„ì„ ì„±ê³µ: {len(ssd_result.get('detections', []))}ê°œ íƒì§€")
                except Exception as e:
                    logger.error(f"SSD ë¶„ì„ ì˜¤ë¥˜: {e}")
                    logger.error(traceback.format_exc())

            return analysis_results

        except Exception as e:
            logger.error(f"DICOM ë¶„ì„ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return [self._create_error_result(str(e))]

    def _create_mock_result(self, modality, skipped=False):
        return [{
            'success': True,
            'detections': [],
            'message': f"{modality} ëª¨ë‹¬ë¦¬í‹° ë¶„ì„ ìŠ¤í‚µë¨.",
            'skipped': skipped,
            'metadata': {}
        }]

    def _create_error_result(self, error_msg):
        return [{
            'success': False,
            'error': error_msg,
            'detections': []
        }]

ai_analyzer = AIAnalyzer()

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'healthy', 'timestamp': datetime.now().isoformat()})

@app.route('/analyze/<instance_id>', methods=['POST'])
def analyze_instance(instance_id):
    try:
        logger.info(f"ì¸ìŠ¤í„´ìŠ¤ ë¶„ì„ ìš”ì²­: {instance_id}")
        instance_info = get_instance_info(instance_id)
        if not instance_info:
            return jsonify({'error': 'Instance not found'}), 404

        dicom_data = get_dicom_file(instance_id)
        if not dicom_data:
            return jsonify({'error': 'DICOM download failed'}), 500

        modality = instance_info['MainDicomTags'].get('Modality', 'UNKNOWN')
        
        # ğŸ”¥ ê°œë³„ ëª¨ë¸ ë¶„ì„ ê²°ê³¼ ë°›ê¸° (ì´ì œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
        analysis_results = ai_analyzer.analyze_dicom_data(dicom_data, modality)
        
        # ğŸ”¥ í•´ìƒë„ ì •ë³´ë¥¼ ë¯¸ë¦¬ ì¶”ì¶œ
        image_width, image_height = get_image_dimensions_from_data(dicom_data)
        
         # ğŸ”¥ ê° ëª¨ë¸ ê²°ê³¼ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì €ì¥ (í•´ìƒë„ ì •ë³´ ì „ë‹¬)
        saved_results = []
        for result in analysis_results:
            if result.get('success') and not result.get('skipped'):
                # ğŸ”¥ í•´ìƒë„ ì •ë³´ë¥¼ resultì— ì¶”ê°€
                result['metadata'] = result.get('metadata', {})
                result['metadata']['image_width'] = image_width
                result['metadata']['image_height'] = image_height
                
                save_result = save_analysis_result(instance_id, result, dicom_data)
                saved_results.append(save_result)
        
        # ì‘ë‹µì€ ëª¨ë“  ëª¨ë¸ ê²°ê³¼ë¥¼ í¬í•¨
        return jsonify({
            'success': True,
            'total_models': len(analysis_results),
            'saved_count': len(saved_results),
            'results': analysis_results,
            'image_dimensions': f"{image_width}x{image_height}"
        })
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

# ğŸ”¥ ìƒˆë¡œìš´ í•¨ìˆ˜: DICOM ë°ì´í„°ì—ì„œ ì§ì ‘ í•´ìƒë„ ì¶”ì¶œ
def get_image_dimensions_from_data(dicom_data):
    """DICOM ë°”ì´ë„ˆë¦¬ ë°ì´í„°ì—ì„œ í•´ìƒë„ ì •ë³´ ì¶”ì¶œ"""
    try:
        import pydicom
        import io
        
        dicom_file = pydicom.dcmread(io.BytesIO(dicom_data))
        
        # Rows, Columns íƒœê·¸ì—ì„œ í•´ìƒë„ ì¶”ì¶œ
        height = getattr(dicom_file, 'Rows', None)
        width = getattr(dicom_file, 'Columns', None)
        
        if height and width:
            logger.info(f"ğŸ“ DICOMì—ì„œ í•´ìƒë„ ì¶”ì¶œ: {width}x{height}")
            return int(width), int(height)
        else:
            logger.warning(f"âš ï¸ DICOMì—ì„œ í•´ìƒë„ ì •ë³´ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return 1024, 1024
            
    except Exception as e:
        logger.error(f"âŒ DICOM í•´ìƒë„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return 1024, 1024
    

@app.route('/results/<instance_id>', methods=['GET'])
def get_analysis_result(instance_id):
    try:
        instance_info = get_instance_info(instance_id)
        if not instance_info:
            return jsonify({'error': 'Instance not found'}), 404

        study_uid = instance_info.get('MainDicomTags', {}).get('StudyInstanceUID')
        if not study_uid:
            return jsonify({'error': 'Study UID not found'}), 404

        url = f"{DJANGO_URL}/api/ai/results/save/{study_uid}/"
        logger.info(f"ğŸ“¥ ê²°ê³¼ ì¡°íšŒ URL: {url}")
        resp = requests.get(url, timeout=80)

        if resp.status_code == 200:
            return jsonify(resp.json())

        return jsonify({'error': 'Failed to fetch results', 'details': resp.text}), resp.status_code

    except Exception as e:
        logger.error(f"âŒ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500


def get_instance_info(instance_id):
    try:
        r1 = requests.get(f"{ORTHANC_URL}/instances/{instance_id}", auth=auth_tuple, timeout=80)
        if r1.status_code != 200:
            logger.warning(f"Orthanc ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {instance_id}, Status: {r1.status_code}, Response: {r1.text}")
            return None
        instance_info = r1.json()

        r2 = requests.get(f"{ORTHANC_URL}/instances/{instance_id}/simplified-tags", auth=auth_tuple, timeout=80)
        if r2.status_code == 200:
            tags = r2.json()
            instance_info.setdefault('MainDicomTags', {})
            for key in ['PatientID', 'StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID', 'InstanceNumber', 'Modality']:
                # â— ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸, ê°’ ìì²´ëŠ” ë¹„ì–´ ìˆì–´ë„ ì €ì¥
                if key in tags:
                    instance_info['MainDicomTags'][key] = tags[key]

        # í•„ìˆ˜ UID ê°’ì´ ëˆ„ë½ë˜ë©´ None ë°˜í™˜í•˜ì—¬ ë¶„ì„/ì €ì¥ ì¤‘ë‹¨
        required_keys = ['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID']
        for key in required_keys:
            if not instance_info['MainDicomTags'].get(key):
                logger.warning(f"{key}ê°€ ëˆ„ë½ë¨. Instance ID: {instance_id}")
                return None

        return instance_info

    except Exception as e:
        logger.error(f"get_instance_info ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return None



def get_dicom_file(instance_id):
    try:
        r = requests.get(f"{ORTHANC_URL}/instances/{instance_id}/file", auth=auth_tuple, timeout=80)
        if r.status_code == 200:
            return r.content
        logger.warning(f"DICOM íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {instance_id}, Status: {r.status_code}, Response: {r.text}")
        return None
    except Exception as e:
        logger.error(f"get_dicom_file ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return None

# ğŸ”¥ ìˆ˜ì •ëœ save_analysis_result í•¨ìˆ˜ (dicom_data ë§¤ê°œë³€ìˆ˜ ì¶”ê°€)
def save_analysis_result(instance_id, result_dict, dicom_data=None):
    try:
        # bboxë¥¼ x, y, width, height í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
        def normalize_bbox(bbox_data):
            logger.debug(f"ğŸ” normalize_bbox í˜¸ì¶œ: {bbox_data} (type: {type(bbox_data)})")
            if isinstance(bbox_data, dict):
                # case 1: {'x':..., 'y':..., 'width':..., 'height':...}
                if all(k in bbox_data for k in ['x', 'y', 'width', 'height']):
                    return {
                        "x": int(bbox_data.get("x", 0)),
                        "y": int(bbox_data.get("y", 0)),
                        "width": int(bbox_data.get("width", 0)),
                        "height": int(bbox_data.get("height", 0)),
                    }
                # case 2: {'x1':..., 'y1':..., 'x2':..., 'y2':...}
                elif all(k in bbox_data for k in ['x1', 'y1', 'x2', 'y2']):
                    x1 = bbox_data.get("x1", 0)
                    y1 = bbox_data.get("y1", 0)
                    x2 = bbox_data.get("x2", 0)
                    y2 = bbox_data.get("y2", 0)
                    return {
                        "x": int(x1),
                        "y": int(y1),
                        "width": int(x2 - x1),
                        "height": int(y2 - y1),
                    }
            # case 3: [x1, y1, x2, y2] (list/tuple)
            elif isinstance(bbox_data, (list, tuple)) and len(bbox_data) == 4:
                x1, y1, x2, y2 = bbox_data
                return {
                    "x": int(x1),
                    "y": int(y1),
                    "width": int(x2 - x1),
                    "height": int(y2 - y1),
                }
            # Torchvision Tensor (ì˜ˆ: torch.Tensor([[x1,y1,x2,y2]]))
            elif hasattr(bbox_data, 'tolist') and isinstance(bbox_data.tolist(), list) and len(bbox_data.tolist()) == 4:
                x1, y1, x2, y2 = bbox_data.tolist()
                logger.debug(f"ğŸ” bbox_dataê°€ torch.Tensor ë¼ì´í¬ ê°ì²´ì„: {x1, y1, x2, y2}")
                return {
                    "x": int(x1),
                    "y": int(y1),
                    "width": int(x2 - x1),
                    "height": int(y2 - y1),
                }
            
            # ëª¨ë“  ê²½ìš°ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
            logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” bbox í˜•ì‹: {bbox_data} (type: {type(bbox_data)}). ê¸°ë³¸ê°’ ì‚¬ìš©.")
            return {"x": 0, "y": 0, "width": 0, "height": 0}
        
        # ğŸ”¥ í•´ìƒë„ ì •ë³´ íšë“ (ì´ë¯¸ ì¶”ì¶œëœ ê²ƒ ìš°ì„  ì‚¬ìš©)
        model_width = result_dict.get("metadata", {}).get("image_width")
        model_height = result_dict.get("metadata", {}).get("image_height")
        
        # ğŸ”¥ DICOM ë°ì´í„°ê°€ ì „ë‹¬ëœ ê²½ìš°ì—ë§Œ ì¶”ì¶œ (fallback)
        dicom_width, dicom_height = None, None
        if not model_width or not model_height:
            if dicom_data:
                dicom_width, dicom_height = get_image_dimensions_from_data(dicom_data)
            
        # ìš°ì„ ìˆœìœ„: ëª¨ë¸ ì œê³µ > DICOM ì¶”ì¶œ > ê¸°ë³¸ê°’
        final_width = model_width or dicom_width or 1024
        final_height = model_height or dicom_height or 1024
        
        logger.info(f"ğŸ“ ìµœì¢… í•´ìƒë„: {final_width}x{final_height} (ëª¨ë¸:{model_width}x{model_height}, DICOM:{dicom_width}x{dicom_height})")

        detections = []
        raw_detections = result_dict.get("detections", [])

        logger.debug(f"ğŸ” save_analysis_result ì‹œì‘. raw_detections íƒ€ì…: {type(raw_detections)}, ë‚´ìš©: {raw_detections}")

        # raw_detectionsê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ê³ , ì•„ë‹ˆë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        if not isinstance(raw_detections, list):
            if isinstance(raw_detections, str):
                try:
                    raw_detections = json.loads(raw_detections)
                    logger.debug(f"âœ… raw_detections ë¬¸ìì—´ â†’ dict íŒŒì‹±ë¨: {raw_detections}")
                    if not isinstance(raw_detections, list):
                         logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ JSON íŒŒì‹± í›„ì—ë„ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ì‹¤ì œ íƒ€ì…: {type(raw_detections)}")
                         raw_detections = []
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©°, JSON íŒŒì‹± ì‹¤íŒ¨: {e} - ì›ë³¸: '{raw_detections}'")
                    raw_detections = []
                except Exception as e:
                    logger.error(f"âŒ 'detections' í‚¤ ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì‹± ì˜¤ë¥˜: {e} - ì›ë³¸: '{raw_detections}'")
                    raw_detections = []
            else:
                logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ì‹¤ì œ íƒ€ì…: {type(raw_detections)}. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”.")
                raw_detections = []

        for i, item in enumerate(raw_detections):
            logger.debug(f"ğŸ“¦ í˜„ì¬ {i}ë²ˆì§¸ item: {item} (type: {type(item)})")

            # ë¬¸ìì—´ì¼ ê²½ìš° JSON íŒŒì‹± ì‹œë„ (ê° itemë³„ë¡œ)
            if isinstance(item, str):
                try:
                    item = json.loads(item)
                    logger.debug(f"âœ… {i}ë²ˆì§¸ item ë¬¸ìì—´ â†’ dict íŒŒì‹±ë¨: {item}")
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ {i}ë²ˆì§¸ item JSON íŒŒì‹± ì‹¤íŒ¨: {e} - ì›ë³¸ ë¬¸ìì—´: '{item}'")
                    continue
                except Exception as e:
                    logger.error(f"âŒ {i}ë²ˆì§¸ item ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì‹± ì˜¤ë¥˜: {e} - ì›ë³¸ ë¬¸ìì—´: '{item}'")
                    continue

            elif hasattr(item, 'keys') and callable(getattr(item, 'keys')):
                logger.debug(f"âœ… {i}ë²ˆì§¸ itemì€ dictì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤. í‚¤ í™•ì¸: {list(item.keys())}")
            elif hasattr(item, 'boxes') and hasattr(item, 'labels') and hasattr(item, 'scores'):
                logger.debug(f"âœ… {i}ë²ˆì§¸ itemì€ ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ ê°ì²´ (boxes, labels, scores í¬í•¨).")
                continue

            # ì—¬ì „íˆ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë©´ ë¬´ì‹œ
            if not isinstance(item, dict):
                logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì´ ìµœì¢…ì ìœ¼ë¡œ dictê°€ ì•„ë‹˜, ë¬´ì‹œ: {item} (type: {type(item)})")
                continue

            # bbox ì²˜ë¦¬ (normalize_bbox í•¨ìˆ˜ ì‚¬ìš©)
            normalized_bbox = normalize_bbox(item.get("bbox"))
            if normalized_bbox is None: 
                 logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì˜ bbox ì •ê·œí™” ì‹¤íŒ¨. ê¸°ë³¸ê°’ ì‚¬ìš©.")
                 normalized_bbox = {"x": 0, "y": 0, "width": 0, "height": 0}

            # í´ë˜ìŠ¤ ì´ë¦„, í™•ì‹ ë„, ì„¤ëª… í•„ë“œ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
            class_name = item.get("label") or item.get("class_name", "")
            confidence = item.get("confidence_score") or item.get("confidence", 0.0)
            description = item.get("ai_text") or item.get("description", "")
            
            # Ensure confidence is float
            try:
                confidence = float(confidence)
            except (ValueError, TypeError):
                confidence = 0.0
                logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì˜ confidence ê°’ì´ ìˆ«ìê°€ ì•„ë‹˜: {item.get('confidence_score') or item.get('confidence')}. 0.0ìœ¼ë¡œ ì„¤ì •.")

            detections.append({
                "class_name": class_name, 
                "confidence": confidence,
                "bbox": normalized_bbox,
                "description": description
            })

        # ğŸ”¥ í•´ìƒë„ ì •ë³´ë¥¼ í¬í•¨í•œ payload ìƒì„±
        payload = {
            "instance_id": instance_id,
            "model_type": result_dict.get("model_type", "unknown"),
            "result": {
                "detections": detections,
                "metadata": {
                    **result_dict.get("metadata", {}),
                    # ğŸ”¥ í•´ìƒë„ ì •ë³´ ì¶”ê°€
                    "image_width": final_width,
                    "image_height": final_height
                },
                "processing_time": float(result_dict.get("processing_time", 0.0)),
                # ğŸ”¥ ìµœìƒìœ„ ë ˆë²¨ì—ë„ í•´ìƒë„ ì •ë³´ ì¶”ê°€ (Django API í˜¸í™˜ì„±)
                "image_width": final_width,
                "image_height": final_height
            }
        }

        url = f"{DJANGO_URL}/api/ai/results/save/"
        headers = {'Content-Type': 'application/json'}

        logger.info(f"ğŸ“¡ Djangoì— {result_dict.get('model_type', 'unknown')} ë¶„ì„ ê²°ê³¼ ì „ì†¡: {instance_id} (í•´ìƒë„: {final_width}x{final_height})")
        resp = requests.post(url, json=payload, headers=headers, timeout=15)

        if resp.status_code in [200, 201]:
            try:
                response_json = resp.json()
                count = response_json.get('count', 0)
                logger.info(f"âœ… {result_dict.get('model_type', 'unknown')} ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {count}ê±´, ì‘ë‹µ: {response_json}")
                return response_json
            except json.JSONDecodeError:
                logger.info(f"âœ… {result_dict.get('model_type', 'unknown')} ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ, Django ì‘ë‹µì€ JSON í˜•ì‹ì´ ì•„ë‹˜: {resp.text}")
                return {"status": "success", "message": resp.text}
        else:
            logger.warning(f"âš ï¸ Django ì „ì†¡ ì‹¤íŒ¨: {resp.status_code} - {resp.text}")
            return {"status": "error", "message": resp.text}

    except Exception as e:
        logger.error(f"ğŸ”¥ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        logger.error(traceback.format_exc())
        return {"status": "error", "message": str(e)}


# def save_analysis_result(instance_id, result_dict):
#     try:
#         # bboxë¥¼ x, y, width, height í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
#         def normalize_bbox(bbox_data):
#             logger.debug(f"ğŸ” normalize_bbox í˜¸ì¶œ: {bbox_data} (type: {type(bbox_data)})")
#             if isinstance(bbox_data, dict):
#                 # case 1: {'x':..., 'y':..., 'width':..., 'height':...}
#                 if all(k in bbox_data for k in ['x', 'y', 'width', 'height']):
#                     return {
#                         "x": int(bbox_data.get("x", 0)),
#                         "y": int(bbox_data.get("y", 0)),
#                         "width": int(bbox_data.get("width", 0)),
#                         "height": int(bbox_data.get("height", 0)),
#                     }
#                 # case 2: {'x1':..., 'y1':..., 'x2':..., 'y2':...}
#                 elif all(k in bbox_data for k in ['x1', 'y1', 'x2', 'y2']):
#                     x1 = bbox_data.get("x1", 0)
#                     y1 = bbox_data.get("y1", 0)
#                     x2 = bbox_data.get("x2", 0)
#                     y2 = bbox_data.get("y2", 0)
#                     return {
#                         "x": int(x1),
#                         "y": int(y1),
#                         "width": int(x2 - x1),
#                         "height": int(y2 - y1),
#                     }
#             # case 3: [x1, y1, x2, y2] (list/tuple)
#             elif isinstance(bbox_data, (list, tuple)) and len(bbox_data) == 4:
#                 x1, y1, x2, y2 = bbox_data
#                 return {
#                     "x": int(x1),
#                     "y": int(y1),
#                     "width": int(x2 - x1),
#                     "height": int(y2 - y1),
#                 }
#             # Torchvision Tensor (ì˜ˆ: torch.Tensor([[x1,y1,x2,y2]]))
#             elif hasattr(bbox_data, 'tolist') and isinstance(bbox_data.tolist(), list) and len(bbox_data.tolist()) == 4:
#                 x1, y1, x2, y2 = bbox_data.tolist()
#                 logger.debug(f"ğŸ” bbox_dataê°€ torch.Tensor ë¼ì´í¬ ê°ì²´ì„: {x1, y1, x2, y2}")
#                 return {
#                     "x": int(x1),
#                     "y": int(y1),
#                     "width": int(x2 - x1),
#                     "height": int(y2 - y1),
#                 }
            
#             # ëª¨ë“  ê²½ìš°ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
#             logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” bbox í˜•ì‹: {bbox_data} (type: {type(bbox_data)}). ê¸°ë³¸ê°’ ì‚¬ìš©.")
#             return {"x": 0, "y": 0, "width": 0, "height": 0}


#         detections = []
#         raw_detections = result_dict.get("detections", [])

#         logger.debug(f"ğŸ” save_analysis_result ì‹œì‘. raw_detections íƒ€ì…: {type(raw_detections)}, ë‚´ìš©: {raw_detections}")

#         # raw_detectionsê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ê³ , ì•„ë‹ˆë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
#         if not isinstance(raw_detections, list):
#             # ë§Œì•½ raw_detections ìì²´ê°€ ë¬¸ìì—´ JSONì´ë¼ë©´ íŒŒì‹± ì‹œë„
#             if isinstance(raw_detections, str):
#                 try:
#                     raw_detections = json.loads(raw_detections)
#                     logger.debug(f"âœ… raw_detections ë¬¸ìì—´ â†’ dict íŒŒì‹±ë¨: {raw_detections}")
#                     if not isinstance(raw_detections, list): # íŒŒì‹± í›„ì—ë„ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ì˜¤ë¥˜
#                          logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ JSON íŒŒì‹± í›„ì—ë„ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ì‹¤ì œ íƒ€ì…: {type(raw_detections)}")
#                          raw_detections = []
#                 except json.JSONDecodeError as e:
#                     logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©°, JSON íŒŒì‹± ì‹¤íŒ¨: {e} - ì›ë³¸: '{raw_detections}'")
#                     raw_detections = [] # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í•´ë‹¹ í•­ëª© ê±´ë„ˆë›°ê¸°
#                 except Exception as e:
#                     logger.error(f"âŒ 'detections' í‚¤ ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì‹± ì˜¤ë¥˜: {e} - ì›ë³¸: '{raw_detections}'")
#                     raw_detections = []
#             else:
#                 logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ì‹¤ì œ íƒ€ì…: {type(raw_detections)}. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”.")
#                 raw_detections = [] # ì•ˆì „í•˜ê²Œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”


#         for i, item in enumerate(raw_detections):
#             logger.debug(f"ğŸ“¦ í˜„ì¬ {i}ë²ˆì§¸ item: {item} (type: {type(item)})")

#             # ë¬¸ìì—´ì¼ ê²½ìš° JSON íŒŒì‹± ì‹œë„ (ê° itemë³„ë¡œ)
#             if isinstance(item, str):
#                 try:
#                     item = json.loads(item)
#                     logger.debug(f"âœ… {i}ë²ˆì§¸ item ë¬¸ìì—´ â†’ dict íŒŒì‹±ë¨: {item}")
#                 except json.JSONDecodeError as e:
#                     logger.error(f"âŒ {i}ë²ˆì§¸ item JSON íŒŒì‹± ì‹¤íŒ¨: {e} - ì›ë³¸ ë¬¸ìì—´: '{item}'")
#                     continue # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í•´ë‹¹ í•­ëª© ê±´ë„ˆë›°ê¸°
#                 except Exception as e:
#                     logger.error(f"âŒ {i}ë²ˆì§¸ item ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì‹± ì˜¤ë¥˜: {e} - ì›ë³¸ ë¬¸ìì—´: '{item}'")
#                     continue
#             # Torchvisionì˜ BoxList ê°™ì€ ê°ì²´ê°€ ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
#             # ì´ ê²½ìš° dictì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë‹¤ë¦…ë‹ˆë‹¤.
#             elif hasattr(item, 'keys') and callable(getattr(item, 'keys')): # dictì²˜ëŸ¼ í‚¤ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê°ì²´
#                 logger.debug(f"âœ… {i}ë²ˆì§¸ itemì€ dictì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤. í‚¤ í™•ì¸: {list(item.keys())}")
#             elif hasattr(item, 'boxes') and hasattr(item, 'labels') and hasattr(item, 'scores'):
#                 # SSDë‚˜ YOLOì˜ ì›ì‹œ ì¶œë ¥ ê°ì²´ê°€ ì§ì ‘ ë„˜ì–´ì˜¤ëŠ” ê²½ìš° (torchvision result object)
#                 logger.debug(f"âœ… {i}ë²ˆì§¸ itemì€ ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ ê°ì²´ (boxes, labels, scores í¬í•¨).")
#                 boxes = item.boxes.xyxy.cpu().numpy().tolist() if hasattr(item.boxes, 'xyxy') else []
#                 scores = item.scores.cpu().numpy().tolist() if hasattr(item, 'scores') else []
#                 labels = item.labels.cpu().numpy().tolist() if hasattr(item, 'labels') else []

#                 # ì´ ê²½ìš° item ìì²´ë¥¼ ë³€í™˜í•˜ì—¬ ì‚¬ìš©
#                 if boxes and scores and labels:
#                     temp_items = []
#                     for b, s, l in zip(boxes, scores, labels):
#                         # SSD/YOLOì—ì„œ ë„˜ì–´ì˜¨ label_idë¥¼ ai_serviceì˜ class_namesì— ë§ì¶° ë³€í™˜ (ì„ì‹œ)
#                         # ai_service.pyì˜ MEDICAL_CLASSESëŠ” 0~13ê¹Œì§€ ìˆìŒ.
#                         # ëª¨ë¸ì—ì„œ ë„˜ì–´ì˜¤ëŠ” label_idì™€ MEDICAL_CLASSESì˜ IDê°€ ì¼ì¹˜í•´ì•¼ í•¨.
#                         # ì—¬ê¸°ì„œëŠ” SSD/YOLOì˜ _get_class_namesì™€ ai_serviceì˜ MEDICAL_CLASSESê°€ ë™ì¼í•˜ë‹¤ê³  ê°€ì •
#                         class_name = AIAnalyzer.MEDICAL_CLASSES.get(int(l), f'Unknown_class_{int(l)}')
                        
#                         temp_items.append({
#                             'bbox': b, # [x1, y1, x2, y2]
#                             'confidence': s,
#                             'label': class_name,
#                             'confidence_score': s,
#                             'ai_text': f"{class_name} ({s:.3f})"
#                         })
#                     item = temp_items # itemì´ ì´ì œ list of dictsê°€ ë¨. for ë£¨í”„ë¥¼ ë‹¤ì‹œ ëŒë ¤ì•¼ í•¨.
#                     logger.debug(f"ğŸ”„ ì›ì‹œ ì¶œë ¥ ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜. ì¬ì²˜ë¦¬ í•„ìš”.")
#                     # ì¬ê·€ í˜¸ì¶œ ë˜ëŠ” ì´ ë¶€ë¶„ì„ ë°˜ë³µë¬¸ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë¡œì§ í•„ìš”
#                     # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ í˜„ì¬ itemì´ ë¦¬ìŠ¤íŠ¸ê°€ ë˜ì—ˆìœ¼ë¯€ë¡œ, ì´ ë£¨í”„ë¥¼ ë‹¤ì‹œ ì‹œì‘í•´ì•¼ í•¨.
#                     # ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ì´ ë£¨í”„ë¥¼ ë‹¤ì‹œ ëŒë¦¬ê±°ë‚˜, ì´ì „ì— `extend` ëŒ€ì‹  `append`ë¥¼ ì¼ë˜ ë¡œì§ìœ¼ë¡œ.
#                     # í˜„ì¬ êµ¬ì¡°ì—ì„œëŠ” ì´ ë¶€ë¶„ì„ ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ì›€.
#                     # ì´ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´, models/ssd/ssd_inference.pyì™€ models/yolov8/yolov8_inference.py ì—ì„œ
#                     # analyze í•¨ìˆ˜ê°€ ë°˜ë“œì‹œ 'detections' ë¦¬ìŠ¤íŠ¸ ì•ˆì— dict í˜•íƒœë¡œ ë°˜í™˜í•˜ë„ë¡ ê°•ì œí•´ì•¼ í•©ë‹ˆë‹¤.
#                     # ì´ê²ƒì´ í˜„ì¬ ai_service.pyì—ì„œ ì§ì ‘ ì²˜ë¦¬í•˜ê¸° í˜ë“  ë¶€ë¶„ì…ë‹ˆë‹¤.
#                     continue # ì„ì‹œ ë°©í¸ìœ¼ë¡œ ê±´ë„ˆë›°ê³  ê²½ê³  ë©”ì‹œì§€ ë‚¨ê¸°ê¸°.

#             # ì—¬ì „íˆ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë©´ ë¬´ì‹œ (ì¬ê·€ì  ì²˜ë¦¬ í›„ ë‚¨ì€ ê²ƒ)
#             if not isinstance(item, dict):
#                 logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì´ ìµœì¢…ì ìœ¼ë¡œ dictê°€ ì•„ë‹˜, ë¬´ì‹œ: {item} (type: {type(item)})")
#                 continue

#             # bbox ì²˜ë¦¬ (normalize_bbox í•¨ìˆ˜ ì‚¬ìš©)
#             normalized_bbox = normalize_bbox(item.get("bbox"))
#             # normalize_bbox í•¨ìˆ˜ì—ì„œ ì´ë¯¸ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•˜ë¯€ë¡œ None ì²´í¬ëŠ” ë¶ˆí•„ìš”í•˜ì§€ë§Œ ìœ ì§€
#             if normalized_bbox is None: 
#                  logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì˜ bbox ì •ê·œí™” ì‹¤íŒ¨. ê¸°ë³¸ê°’ ì‚¬ìš©.")
#                  normalized_bbox = {"x": 0, "y": 0, "width": 0, "height": 0}

#             # í´ë˜ìŠ¤ ì´ë¦„, í™•ì‹ ë„, ì„¤ëª… í•„ë“œ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
#             class_name = item.get("label") or item.get("class_name", "")
#             confidence = item.get("confidence_score") or item.get("confidence", 0.0)
#             description = item.get("ai_text") or item.get("description", "")
            
#             # Ensure confidence is float
#             try:
#                 confidence = float(confidence)
#             except (ValueError, TypeError):
#                 confidence = 0.0
#                 logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì˜ confidence ê°’ì´ ìˆ«ìê°€ ì•„ë‹˜: {item.get('confidence_score') or item.get('confidence')}. 0.0ìœ¼ë¡œ ì„¤ì •.")

#             detections.append({
#                 "class_name": class_name, 
#                 "confidence": confidence,
#                 "bbox": normalized_bbox,
#                 "description": description
#             })

#         payload = {
#             "instance_id": instance_id,
#             "result": {
#                 "detections": detections,
#                 "metadata": {
#                     "model_used": result_dict.get("metadata", {}).get("model_used", "unknown"),
#                     "model_version": result_dict.get("metadata", {}).get("model_version", "v1.0")
#                 },
#                 "processing_time": float(result_dict.get("processing_time", 0.0))
#             }
#         }

#         url = f"{DJANGO_URL}/api/ai/results/save/"
#         headers = {'Content-Type': 'application/json'}

#         logger.info(f"ğŸ“¡ Djangoì— ë¶„ì„ ê²°ê³¼ ì „ì†¡: {instance_id}")
#         resp = requests.post(url, json=payload, headers=headers, timeout=15)

#         if resp.status_code in [200, 201]:
#             try:
#                 # ì‘ë‹µì´ JSONì´ ì•„ë‹ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì²˜ë¦¬
#                 response_json = resp.json()
#                 count = response_json.get('count', 0)
#                 logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {count}ê±´, ì‘ë‹µ: {response_json}")
#             except json.JSONDecodeError:
#                 logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ, Django ì‘ë‹µì€ JSON í˜•ì‹ì´ ì•„ë‹˜: {resp.text}")
#         else:
#             logger.warning(f"âš ï¸ Django ì „ì†¡ ì‹¤íŒ¨: {resp.status_code} - {resp.text}")

#     except Exception as e:
#         logger.error(f"ğŸ”¥ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
#         logger.error(traceback.format_exc())


# def analyze_single_instance(instance_id):
#     instance_info = get_instance_info(instance_id)
#     if not instance_info:
#         raise Exception("Instance not found")

#     dicom_data = get_dicom_file(instance_id)
#     if not dicom_data:
#         raise Exception("DICOM file download failed")

#     modality = instance_info['MainDicomTags'].get('Modality', 'UNKNOWN')
#     result = ai_analyzer.analyze_dicom_data(dicom_data, modality)
    
#     if not result or not result.get('success'):
#         raise Exception(f"AI analysis failed: {result.get('error', 'unknown')}")

#     save_analysis_result(instance_id, result)
#     return result


# ìˆ˜ì •ëœ analyze_single_instance í•¨ìˆ˜

def analyze_single_instance(instance_id):
    instance_info = get_instance_info(instance_id)
    if not instance_info:
        raise Exception("Instance not found")

    dicom_data = get_dicom_file(instance_id)
    if not dicom_data:
        raise Exception("DICOM file download failed")

    # í•´ìƒë„ ì •ë³´ ë¯¸ë¦¬ ì¶”ì¶œ
    image_width, image_height = get_image_dimensions_from_data(dicom_data)

    modality = instance_info['MainDicomTags'].get('Modality', 'UNKNOWN')
    results = ai_analyzer.analyze_dicom_data(dicom_data, modality)
    
    # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸ (analyze_dicom_dataëŠ” ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
    if not isinstance(results, list):
        results = [results]
    
    # ê° ê²°ê³¼ì— í•´ìƒë„ ì •ë³´ ì¶”ê°€í•˜ê³  ì €ì¥
    for result in results:
        if result and result.get('success'):
            # í•´ìƒë„ ì •ë³´ ì¶”ê°€
            result['metadata'] = result.get('metadata', {})
            result['metadata']['image_width'] = image_width
            result['metadata']['image_height'] = image_height
            
            save_analysis_result(instance_id, result, dicom_data)
        else:
            raise Exception(f"AI analysis failed: {result.get('error', 'unknown') if result else 'No result'}")

    return results[0] if len(results) == 1 else results