import json
import sys
import os
import logging
from datetime import datetime
import importlib.util
import requests
from flask import Flask, request, jsonify
import pydicom
import io
import base64
import time
import traceback

# ë¡œê¹… ì„¤ì •
# StreamHandlerì— ì¸ì½”ë”©ì„ ëª…ì‹œì ìœ¼ë¡œ 'utf-8'ë¡œ ì„¤ì •í•˜ì—¬ í•œê¸€ ê¹¨ì§ ë° UnicodeEncodeError ë°©ì§€
log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

file_handler = logging.FileHandler('/var/log/ai_service.log', encoding='utf-8')
file_handler.setFormatter(log_formatter)

stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setFormatter(log_formatter)
stream_handler.encoding = 'utf-8' # <-- ì´ ë¶€ë¶„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

logging.basicConfig(
    level=logging.INFO, # ìš´ì˜ í™˜ê²½ì—ì„œëŠ” INFO, ë””ë²„ê¹… ì‹œì—ëŠ” DEBUG
    handlers=[
        file_handler,
        stream_handler
    ]
)
logger = logging.getLogger('AIService')

# Flask ì•± ì´ˆê¸°í™”
app = Flask(__name__)

# ê²½ë¡œ ë° ì„œë²„ ì„¤ì •
MODELS_PATH = '/models'
YOLO_MODEL_PATH = os.path.join(MODELS_PATH, 'yolov8', 'yolov8_inference.py')
SSD_MODEL_PATH = os.path.join(MODELS_PATH, 'ssd', 'ssd_inference.py')
ORTHANC_URL = os.getenv("ORTHANC_URL", "http://35.225.63.41:8042")
DJANGO_URL = 'http://35.225.63.41:8000'

ORTHANC_USERNAME = 'orthanc'
ORTHANC_PASSWORD = 'orthanc'

auth_tuple = (ORTHANC_USERNAME, ORTHANC_PASSWORD)

class AIAnalyzer:
    MEDICAL_CLASSES = {
        0: 'Normal', 1: 'Aortic enlargement', 2: 'Atelectasis',
        3: 'Calcification', 4: 'Cardiomegaly', 5: 'Consolidation',
        6: 'ILD', 7: 'Infiltration', 8: 'Lung Opacity',
        9: 'Nodule/Mass', 10: 'Other lesion', 11: 'Pleural effusion',
        12: 'Pleural thickening', 13: 'Pulmonary fibrosis'
    }

    def __init__(self):
        self.yolo_module = None
        self.ssd_module = None
        self._load_models()

    def _load_models(self):
        try:
            if os.path.exists(YOLO_MODEL_PATH):
                spec = importlib.util.spec_from_file_location("yolo_inference", YOLO_MODEL_PATH)
                self.yolo_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(self.yolo_module)
                logger.info(f"YOLO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {YOLO_MODEL_PATH}")
            if os.path.exists(SSD_MODEL_PATH):
                spec = importlib.util.spec_from_file_location("ssd_inference", SSD_MODEL_PATH)
                self.ssd_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(self.ssd_module)
                logger.info(f"SSD ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {SSD_MODEL_PATH}")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.error(traceback.format_exc())

    def analyze_dicom_data(self, dicom_data, modality):
        start_time = time.time()
        try:
            model_type = self._select_model(modality)
            if not model_type:
                return self._create_mock_result(modality, skipped=True)

            result = {
                'success': True,
                'detections': [],
                'yolo_results': {},
                'ssd_results': {},
                'message': '',
                'metadata': {}
            }

            if model_type in ['both', 'yolo'] and self.yolo_module:
                try:
                    yolo_result = self.yolo_module.analyze(dicom_data)
                    if yolo_result.get('success'):
                        result['yolo_results'] = yolo_result
                        # YOLO ê²°ê³¼ì˜ detectionsê°€ ì˜¬ë°”ë¥¸ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                        if isinstance(yolo_result.get('detections'), list):
                            result['detections'].extend(yolo_result.get('detections', []))
                        else:
                            logger.warning("YOLO detections í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜.")
                        logger.info(f"âœ… YOLO ì‹¤ì œ ë¶„ì„ ì„±ê³µ: {len(yolo_result.get('detections', []))}ê°œ")
                except Exception as e:
                    logger.error(f"YOLO ë¶„ì„ ì˜¤ë¥˜: {e}")
                    logger.error(traceback.format_exc())

            if model_type in ['both', 'ssd'] and self.ssd_module:
                try:
                    ssd_result = self.ssd_module.analyze(dicom_data)
                    if ssd_result.get('success'):
                        result['ssd_results'] = ssd_result
                        # SSD ê²°ê³¼ì˜ detectionsê°€ ì˜¬ë°”ë¥¸ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                        if isinstance(ssd_result.get('detections'), list):
                            result['detections'].extend(ssd_result.get('detections', []))
                        else:
                            logger.warning("SSD detections í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜.")
                        logger.info(f"âœ… SSD ì‹¤ì œ ë¶„ì„ ì„±ê³µ: {len(ssd_result.get('detections', []))}ê°œ")
                except Exception as e:
                    logger.error(f"SSD ë¶„ì„ ì˜¤ë¥˜: {e}")
                    logger.error(traceback.format_exc())

            processing_time = time.time() - start_time
            result['metadata'].update({
                'model_used': model_type,
                'analysis_timestamp': datetime.now().isoformat(),
                'modality': modality,
                'processing_time': processing_time,
                'model_version': 'v1.0'
            })
            result['processing_time'] = processing_time
            return result

        except Exception as e:
            logger.error(f"DICOM ë¶„ì„ ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return self._create_error_result(str(e))

    def _select_model(self, modality):
        return 'both' if modality == 'CR' else None

    def _create_mock_result(self, modality, skipped=False):
        return {
            'success': True,
            'detections': [],
            'message': f"{modality} ëª¨ë‹¬ë¦¬í‹° ë¶„ì„ ìŠ¤í‚µë¨.",
            'skipped': skipped,
            'metadata': {}
        }

    def _create_error_result(self, error_msg):
        return {
            'success': False,
            'error': error_msg,
            'detections': []
        }

ai_analyzer = AIAnalyzer()

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'healthy', 'timestamp': datetime.now().isoformat()})

@app.route('/analyze/<instance_id>', methods=['POST'])
def analyze_instance(instance_id):
    try:
        logger.info(f"ì¸ìŠ¤í„´ìŠ¤ ë¶„ì„ ìš”ì²­: {instance_id}")
        instance_info = get_instance_info(instance_id)
        if not instance_info:
            return jsonify({'error': 'Instance not found'}), 404

        dicom_data = get_dicom_file(instance_id)
        if not dicom_data:
            return jsonify({'error': 'DICOM download failed'}), 500

        modality = instance_info['MainDicomTags'].get('Modality', 'UNKNOWN')
        analysis_result = ai_analyzer.analyze_dicom_data(dicom_data, modality)
        
        # ë¶„ì„ ê²°ê³¼ê°€ ìœ íš¨í•œì§€ ë‹¤ì‹œ í•œë²ˆ í™•ì¸
        if not analysis_result or not analysis_result.get('success'):
            logger.error(f"ë¶„ì„ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì„±ê³µí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {analysis_result.get('error', 'unknown error')}")
            return jsonify({'error': 'AI analysis failed', 'details': analysis_result.get('error', 'unknown')}), 500
            
        save_analysis_result(instance_id, analysis_result)
        return jsonify(analysis_result)
    except Exception as e:
        logger.error(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

@app.route('/results/<instance_id>', methods=['GET'])
def get_analysis_result(instance_id):
    try:
        instance_info = get_instance_info(instance_id)
        if not instance_info:
            return jsonify({'error': 'Instance not found'}), 404

        study_uid = instance_info.get('MainDicomTags', {}).get('StudyInstanceUID')
        if not study_uid:
            return jsonify({'error': 'Study UID not found'}), 404

        url = f"{DJANGO_URL}/api/ai/results/save/{study_uid}/"
        logger.info(f"ğŸ“¥ ê²°ê³¼ ì¡°íšŒ URL: {url}")
        resp = requests.get(url, timeout=80)

        if resp.status_code == 200:
            return jsonify(resp.json())

        return jsonify({'error': 'Failed to fetch results', 'details': resp.text}), resp.status_code

    except Exception as e:
        logger.error(f"âŒ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500


def get_instance_info(instance_id):
    try:
        r1 = requests.get(f"{ORTHANC_URL}/instances/{instance_id}", auth=auth_tuple, timeout=80)
        if r1.status_code != 200:
            logger.warning(f"Orthanc ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {instance_id}, Status: {r1.status_code}, Response: {r1.text}")
            return None
        instance_info = r1.json()

        r2 = requests.get(f"{ORTHANC_URL}/instances/{instance_id}/simplified-tags", auth=auth_tuple, timeout=80)
        if r2.status_code == 200:
            tags = r2.json()
            instance_info.setdefault('MainDicomTags', {})
            for key in ['PatientID', 'StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID', 'InstanceNumber', 'Modality']:
                # â— ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸, ê°’ ìì²´ëŠ” ë¹„ì–´ ìˆì–´ë„ ì €ì¥
                if key in tags:
                    instance_info['MainDicomTags'][key] = tags[key]

        # í•„ìˆ˜ UID ê°’ì´ ëˆ„ë½ë˜ë©´ None ë°˜í™˜í•˜ì—¬ ë¶„ì„/ì €ì¥ ì¤‘ë‹¨
        required_keys = ['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID']
        for key in required_keys:
            if not instance_info['MainDicomTags'].get(key):
                logger.warning(f"{key}ê°€ ëˆ„ë½ë¨. Instance ID: {instance_id}")
                return None

        return instance_info

    except Exception as e:
        logger.error(f"get_instance_info ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return None



def get_dicom_file(instance_id):
    try:
        r = requests.get(f"{ORTHANC_URL}/instances/{instance_id}/file", auth=auth_tuple, timeout=80)
        if r.status_code == 200:
            return r.content
        logger.warning(f"DICOM íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {instance_id}, Status: {r.status_code}, Response: {r.text}")
        return None
    except Exception as e:
        logger.error(f"get_dicom_file ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        return None

def save_analysis_result(instance_id, result_dict):
    try:
        # bboxë¥¼ x, y, width, height í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
        def normalize_bbox(bbox_data):
            logger.debug(f"ğŸ” normalize_bbox í˜¸ì¶œ: {bbox_data} (type: {type(bbox_data)})")
            if isinstance(bbox_data, dict):
                # case 1: {'x':..., 'y':..., 'width':..., 'height':...}
                if all(k in bbox_data for k in ['x', 'y', 'width', 'height']):
                    return {
                        "x": int(bbox_data.get("x", 0)),
                        "y": int(bbox_data.get("y", 0)),
                        "width": int(bbox_data.get("width", 0)),
                        "height": int(bbox_data.get("height", 0)),
                    }
                # case 2: {'x1':..., 'y1':..., 'x2':..., 'y2':...}
                elif all(k in bbox_data for k in ['x1', 'y1', 'x2', 'y2']):
                    x1 = bbox_data.get("x1", 0)
                    y1 = bbox_data.get("y1", 0)
                    x2 = bbox_data.get("x2", 0)
                    y2 = bbox_data.get("y2", 0)
                    return {
                        "x": int(x1),
                        "y": int(y1),
                        "width": int(x2 - x1),
                        "height": int(y2 - y1),
                    }
            # case 3: [x1, y1, x2, y2] (list/tuple)
            elif isinstance(bbox_data, (list, tuple)) and len(bbox_data) == 4:
                x1, y1, x2, y2 = bbox_data
                return {
                    "x": int(x1),
                    "y": int(y1),
                    "width": int(x2 - x1),
                    "height": int(y2 - y1),
                }
            # Torchvision Tensor (ì˜ˆ: torch.Tensor([[x1,y1,x2,y2]]))
            elif hasattr(bbox_data, 'tolist') and isinstance(bbox_data.tolist(), list) and len(bbox_data.tolist()) == 4:
                x1, y1, x2, y2 = bbox_data.tolist()
                logger.debug(f"ğŸ” bbox_dataê°€ torch.Tensor ë¼ì´í¬ ê°ì²´ì„: {x1, y1, x2, y2}")
                return {
                    "x": int(x1),
                    "y": int(y1),
                    "width": int(x2 - x1),
                    "height": int(y2 - y1),
                }
            
            # ëª¨ë“  ê²½ìš°ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
            logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” bbox í˜•ì‹: {bbox_data} (type: {type(bbox_data)}). ê¸°ë³¸ê°’ ì‚¬ìš©.")
            return {"x": 0, "y": 0, "width": 0, "height": 0}


        detections = []
        raw_detections = result_dict.get("detections", [])

        logger.debug(f"ğŸ” save_analysis_result ì‹œì‘. raw_detections íƒ€ì…: {type(raw_detections)}, ë‚´ìš©: {raw_detections}")

        # raw_detectionsê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸í•˜ê³ , ì•„ë‹ˆë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        if not isinstance(raw_detections, list):
            # ë§Œì•½ raw_detections ìì²´ê°€ ë¬¸ìì—´ JSONì´ë¼ë©´ íŒŒì‹± ì‹œë„
            if isinstance(raw_detections, str):
                try:
                    raw_detections = json.loads(raw_detections)
                    logger.debug(f"âœ… raw_detections ë¬¸ìì—´ â†’ dict íŒŒì‹±ë¨: {raw_detections}")
                    if not isinstance(raw_detections, list): # íŒŒì‹± í›„ì—ë„ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ì˜¤ë¥˜
                         logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ JSON íŒŒì‹± í›„ì—ë„ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ì‹¤ì œ íƒ€ì…: {type(raw_detections)}")
                         raw_detections = []
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©°, JSON íŒŒì‹± ì‹¤íŒ¨: {e} - ì›ë³¸: '{raw_detections}'")
                    raw_detections = [] # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í•´ë‹¹ í•­ëª© ê±´ë„ˆë›°ê¸°
                except Exception as e:
                    logger.error(f"âŒ 'detections' í‚¤ ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì‹± ì˜¤ë¥˜: {e} - ì›ë³¸: '{raw_detections}'")
                    raw_detections = []
            else:
                logger.error(f"âŒ 'detections' í‚¤ì˜ ê°’ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ì‹¤ì œ íƒ€ì…: {type(raw_detections)}. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”.")
                raw_detections = [] # ì•ˆì „í•˜ê²Œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”


        for i, item in enumerate(raw_detections):
            logger.debug(f"ğŸ“¦ í˜„ì¬ {i}ë²ˆì§¸ item: {item} (type: {type(item)})")

            # ë¬¸ìì—´ì¼ ê²½ìš° JSON íŒŒì‹± ì‹œë„ (ê° itemë³„ë¡œ)
            if isinstance(item, str):
                try:
                    item = json.loads(item)
                    logger.debug(f"âœ… {i}ë²ˆì§¸ item ë¬¸ìì—´ â†’ dict íŒŒì‹±ë¨: {item}")
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ {i}ë²ˆì§¸ item JSON íŒŒì‹± ì‹¤íŒ¨: {e} - ì›ë³¸ ë¬¸ìì—´: '{item}'")
                    continue # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í•´ë‹¹ í•­ëª© ê±´ë„ˆë›°ê¸°
                except Exception as e:
                    logger.error(f"âŒ {i}ë²ˆì§¸ item ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì‹± ì˜¤ë¥˜: {e} - ì›ë³¸ ë¬¸ìì—´: '{item}'")
                    continue
            # Torchvisionì˜ BoxList ê°™ì€ ê°ì²´ê°€ ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
            # ì´ ê²½ìš° dictì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë‹¤ë¦…ë‹ˆë‹¤.
            elif hasattr(item, 'keys') and callable(getattr(item, 'keys')): # dictì²˜ëŸ¼ í‚¤ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê°ì²´
                logger.debug(f"âœ… {i}ë²ˆì§¸ itemì€ dictì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤. í‚¤ í™•ì¸: {list(item.keys())}")
            elif hasattr(item, 'boxes') and hasattr(item, 'labels') and hasattr(item, 'scores'):
                # SSDë‚˜ YOLOì˜ ì›ì‹œ ì¶œë ¥ ê°ì²´ê°€ ì§ì ‘ ë„˜ì–´ì˜¤ëŠ” ê²½ìš° (torchvision result object)
                logger.debug(f"âœ… {i}ë²ˆì§¸ itemì€ ëª¨ë¸ì˜ ì›ì‹œ ì¶œë ¥ ê°ì²´ (boxes, labels, scores í¬í•¨).")
                boxes = item.boxes.xyxy.cpu().numpy().tolist() if hasattr(item.boxes, 'xyxy') else []
                scores = item.scores.cpu().numpy().tolist() if hasattr(item, 'scores') else []
                labels = item.labels.cpu().numpy().tolist() if hasattr(item, 'labels') else []

                # ì´ ê²½ìš° item ìì²´ë¥¼ ë³€í™˜í•˜ì—¬ ì‚¬ìš©
                if boxes and scores and labels:
                    temp_items = []
                    for b, s, l in zip(boxes, scores, labels):
                        # SSD/YOLOì—ì„œ ë„˜ì–´ì˜¨ label_idë¥¼ ai_serviceì˜ class_namesì— ë§ì¶° ë³€í™˜ (ì„ì‹œ)
                        # ai_service.pyì˜ MEDICAL_CLASSESëŠ” 0~13ê¹Œì§€ ìˆìŒ.
                        # ëª¨ë¸ì—ì„œ ë„˜ì–´ì˜¤ëŠ” label_idì™€ MEDICAL_CLASSESì˜ IDê°€ ì¼ì¹˜í•´ì•¼ í•¨.
                        # ì—¬ê¸°ì„œëŠ” SSD/YOLOì˜ _get_class_namesì™€ ai_serviceì˜ MEDICAL_CLASSESê°€ ë™ì¼í•˜ë‹¤ê³  ê°€ì •
                        class_name = AIAnalyzer.MEDICAL_CLASSES.get(int(l), f'Unknown_class_{int(l)}')
                        
                        temp_items.append({
                            'bbox': b, # [x1, y1, x2, y2]
                            'confidence': s,
                            'label': class_name,
                            'confidence_score': s,
                            'ai_text': f"{class_name} ({s:.3f})"
                        })
                    item = temp_items # itemì´ ì´ì œ list of dictsê°€ ë¨. for ë£¨í”„ë¥¼ ë‹¤ì‹œ ëŒë ¤ì•¼ í•¨.
                    logger.debug(f"ğŸ”„ ì›ì‹œ ì¶œë ¥ ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜. ì¬ì²˜ë¦¬ í•„ìš”.")
                    # ì¬ê·€ í˜¸ì¶œ ë˜ëŠ” ì´ ë¶€ë¶„ì„ ë°˜ë³µë¬¸ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë¡œì§ í•„ìš”
                    # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ í˜„ì¬ itemì´ ë¦¬ìŠ¤íŠ¸ê°€ ë˜ì—ˆìœ¼ë¯€ë¡œ, ì´ ë£¨í”„ë¥¼ ë‹¤ì‹œ ì‹œì‘í•´ì•¼ í•¨.
                    # ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ì´ ë£¨í”„ë¥¼ ë‹¤ì‹œ ëŒë¦¬ê±°ë‚˜, ì´ì „ì— `extend` ëŒ€ì‹  `append`ë¥¼ ì¼ë˜ ë¡œì§ìœ¼ë¡œ.
                    # í˜„ì¬ êµ¬ì¡°ì—ì„œëŠ” ì´ ë¶€ë¶„ì„ ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ì›€.
                    # ì´ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´, models/ssd/ssd_inference.pyì™€ models/yolov8/yolov8_inference.py ì—ì„œ
                    # analyze í•¨ìˆ˜ê°€ ë°˜ë“œì‹œ 'detections' ë¦¬ìŠ¤íŠ¸ ì•ˆì— dict í˜•íƒœë¡œ ë°˜í™˜í•˜ë„ë¡ ê°•ì œí•´ì•¼ í•©ë‹ˆë‹¤.
                    # ì´ê²ƒì´ í˜„ì¬ ai_service.pyì—ì„œ ì§ì ‘ ì²˜ë¦¬í•˜ê¸° í˜ë“  ë¶€ë¶„ì…ë‹ˆë‹¤.
                    continue # ì„ì‹œ ë°©í¸ìœ¼ë¡œ ê±´ë„ˆë›°ê³  ê²½ê³  ë©”ì‹œì§€ ë‚¨ê¸°ê¸°.

            # ì—¬ì „íˆ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë©´ ë¬´ì‹œ (ì¬ê·€ì  ì²˜ë¦¬ í›„ ë‚¨ì€ ê²ƒ)
            if not isinstance(item, dict):
                logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì´ ìµœì¢…ì ìœ¼ë¡œ dictê°€ ì•„ë‹˜, ë¬´ì‹œ: {item} (type: {type(item)})")
                continue

            # bbox ì²˜ë¦¬ (normalize_bbox í•¨ìˆ˜ ì‚¬ìš©)
            normalized_bbox = normalize_bbox(item.get("bbox"))
            # normalize_bbox í•¨ìˆ˜ì—ì„œ ì´ë¯¸ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•˜ë¯€ë¡œ None ì²´í¬ëŠ” ë¶ˆí•„ìš”í•˜ì§€ë§Œ ìœ ì§€
            if normalized_bbox is None: 
                 logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì˜ bbox ì •ê·œí™” ì‹¤íŒ¨. ê¸°ë³¸ê°’ ì‚¬ìš©.")
                 normalized_bbox = {"x": 0, "y": 0, "width": 0, "height": 0}

            # í´ë˜ìŠ¤ ì´ë¦„, í™•ì‹ ë„, ì„¤ëª… í•„ë“œ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
            class_name = item.get("label") or item.get("class_name", "")
            confidence = item.get("confidence_score") or item.get("confidence", 0.0)
            description = item.get("ai_text") or item.get("description", "")
            
            # Ensure confidence is float
            try:
                confidence = float(confidence)
            except (ValueError, TypeError):
                confidence = 0.0
                logger.warning(f"âš ï¸ {i}ë²ˆì§¸ itemì˜ confidence ê°’ì´ ìˆ«ìê°€ ì•„ë‹˜: {item.get('confidence_score') or item.get('confidence')}. 0.0ìœ¼ë¡œ ì„¤ì •.")

            detections.append({
                "class_name": class_name, 
                "confidence": confidence,
                "bbox": normalized_bbox,
                "description": description
            })

        payload = {
            "instance_id": instance_id,
            "result": {
                "detections": detections,
                "metadata": {
                    "model_used": result_dict.get("metadata", {}).get("model_used", "unknown"),
                    "model_version": result_dict.get("metadata", {}).get("model_version", "v1.0")
                },
                "processing_time": float(result_dict.get("processing_time", 0.0))
            }
        }

        url = f"{DJANGO_URL}/api/ai/results/save/"
        headers = {'Content-Type': 'application/json'}

        logger.info(f"ğŸ“¡ Djangoì— ë¶„ì„ ê²°ê³¼ ì „ì†¡: {instance_id}")
        resp = requests.post(url, json=payload, headers=headers, timeout=15)

        if resp.status_code in [200, 201]:
            try:
                # ì‘ë‹µì´ JSONì´ ì•„ë‹ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì²˜ë¦¬
                response_json = resp.json()
                count = response_json.get('count', 0)
                logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {count}ê±´, ì‘ë‹µ: {response_json}")
            except json.JSONDecodeError:
                logger.info(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ, Django ì‘ë‹µì€ JSON í˜•ì‹ì´ ì•„ë‹˜: {resp.text}")
        else:
            logger.warning(f"âš ï¸ Django ì „ì†¡ ì‹¤íŒ¨: {resp.status_code} - {resp.text}")

    except Exception as e:
        logger.error(f"ğŸ”¥ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        logger.error(traceback.format_exc())

def analyze_single_instance(instance_id):
    instance_info = get_instance_info(instance_id)
    if not instance_info:
        raise Exception("Instance not found")

    dicom_data = get_dicom_file(instance_id)
    if not dicom_data:
        raise Exception("DICOM file download failed")

    modality = instance_info['MainDicomTags'].get('Modality', 'UNKNOWN')
    result = ai_analyzer.analyze_dicom_data(dicom_data, modality)
    
    if not result or not result.get('success'):
        raise Exception(f"AI analysis failed: {result.get('error', 'unknown')}")

    save_analysis_result(instance_id, result)
    return result